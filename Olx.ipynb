{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSnr2oMPb3TkM61xfX3+7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardLouis/IMOB/blob/main/Olx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4j-IHY5dIAI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import requests\n",
        "from bs4 imprt BeautifulSoup\n",
        "import re\n",
        "from datetime import datetime, date"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to map Romanian month names to English ones\n",
        "romanian_to_english_months = {\n",
        "    'ianuarie': 'January',\n",
        "    'februarie': 'February',\n",
        "    'martie': 'March',\n",
        "    'aprilie': 'April',\n",
        "    'mai': 'May',\n",
        "    'iunie': 'June',\n",
        "    'iulie': 'July',\n",
        "    'august': 'August',\n",
        "    'septembrie': 'September',\n",
        "    'octombrie': 'October',\n",
        "    'noiembrie': 'November',\n",
        "    'decembrie': 'December'\n",
        "}"
      ],
      "metadata": {
        "id": "P0iDItrzdOe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_url(nr_camere_filter, etaj_url_filter, supr_min, supr_max, an_const_url_filter):\n",
        "    return f\"https://www.olx.ro/imobiliare/apartamente-garsoniere-de-vanzare/{nr_camere_filter}/iasi-judet/?currency=EUR&search%5Border%5D=created_at:desc&search%5Bfilter_float_m:from%5D={supr_min}&search%5Bfilter_float_m:to%5D={supr_max}{an_const_url_filter}{etaj_url_filter}\"\n",
        "\n",
        "\n",
        "def get_number_of_pages(soup):\n",
        "    pagination_wrapper = soup.find('div', {'data-testid': 'pagination-wrapper'})\n",
        "    if not pagination_wrapper:\n",
        "        return None\n",
        "    pagination_items = pagination_wrapper.find_all('li', {'data-testid': 'pagination-list-item'})\n",
        "\n",
        "    if pagination_items:  # Add this check\n",
        "        return max([int(item.find('a')['data-testid'].split('-')[-1]) for item in pagination_items])\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "W_vf0YHZdYTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info_from_page(soup):\n",
        "    listings = soup.find_all(class_='css-1sw7q4x')\n",
        "    rows = []\n",
        "\n",
        "    for listing in listings:\n",
        "        title = listing.find('h6')\n",
        "        if not title:\n",
        "            continue\n",
        "\n",
        "        link = str(listing.find('a')['href'])\n",
        "        if link.startswith(\"/d/\"):\n",
        "            link = 'https://www.olx.ro'+link\n",
        "\n",
        "        location_date = listing.find(class_='css-veheph').get_text()\n",
        "        location = location_date.split(' - ')[0]\n",
        "        price = listing.find(class_='css-10b0gli').get_text()\n",
        "        price = float(\"\".join(price.split(' ')[:2]))\n",
        "\n",
        "        update_date = location_date.split(' - ')[1]\n",
        "\n",
        "        # Extract and format the 'Update Date' using regular expressions\n",
        "\n",
        "        match = re.search(r'\\d{1,2} \\w+ \\d{4}', update_date)\n",
        "        if match:\n",
        "            date_str = match.group(0)\n",
        "            # Replace Romanian month names with English ones\n",
        "            for ro_month, en_month in romanian_to_english_months.items():\n",
        "                date_str = date_str.replace(ro_month, en_month)\n",
        "            parsed_date = datetime.strptime(date_str, '%d %B %Y')\n",
        "            formatted_date = parsed_date.strftime('%d %B %Y')\n",
        "        elif \"Azi\" in update_date:\n",
        "            formatted_date = date.today()\n",
        "        else:\n",
        "            formatted_date = ''\n",
        "        surface = float(listing.find(\n",
        "            class_='css-643j0o').get_text().split(' ')[0])\n",
        "        \n",
        "        # Calculate the price per square meter\n",
        "        price_per_sqm = price / surface\n",
        "\n",
        "        data = {\n",
        "            'Title': title.get_text(),\n",
        "            'Link': link,\n",
        "            'Location': location,\n",
        "            'Price': price,\n",
        "            'Update Date': formatted_date,\n",
        "            'Surface': surface,\n",
        "            'Price per sqm': price_per_sqm  # Add the new field here\n",
        "        }\n",
        "        rows.append(data)\n",
        "\n",
        "    return rows"
      ],
      "metadata": {
        "id": "OsSwyjSMdgyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(rows, output_file_path):\n",
        "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "        fieldnames = ['Title', 'Link', 'Location', 'Price', 'Update Date', 'Surface', 'Price per sqm']  # Add the new field here\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for row in rows:\n",
        "            writer.writerow(row)"
      ],
      "metadata": {
        "id": "BYIK3P0CeWZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# file\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omS-78ZXftXv",
        "outputId": "212f8c85-4e62-490c-a85b-7366e0fe5754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "script_directory = os.path.dirname(file_path)\n",
        "output_file_path = os.path.join(script_directory, 'output.csv')\n",
        "anConstNameList = [\"inainte-de-1977\", \"1977-1990\", \"1990-2000\", \"dupa-2000\"]\n",
        "an_const_url_list = [\"&search%5Bfilter_enum_constructie%5D%5B{}%5D={}\".format(\n",
        "    xId, x) for xId, x in enumerate(anConstNameList)]\n",
        "\n",
        "\n",
        "etajNameList = [\"parter\", \"fl_1\", \"fl_2\", \"fl_3\", \"fl_4\"]\n",
        "etaj_url_list = [\"&search%5Bfilter_enum_floor%5D%5B{}%5D={}\".format(\n",
        "    xId, x) for xId, x in enumerate(etajNameList)]\n",
        "\n",
        "# Select the needed filters\n",
        "nr_camere_filter = \"2-camere\"\n",
        "an_const_url_filter = \"\".join(an_const_url_list)  # This line filters for \"inainte-de-1977\" and \"1977-1990\"\n",
        "etaj_url_filter = \"\".join(etaj_url_list)  # This line filters for \"parter\" and \"fl_1\"\n",
        "print(an_const_url_filter)\n",
        "supr_min = 35\n",
        "supr_max = 45\n",
        "url = get_url(nr_camere_filter, etaj_url_filter, supr_min, supr_max, an_const_url_filter)\n",
        "response = requests.get(url)\n",
        "print(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZrJbW4qebC-",
        "outputId": "181a8cf1-f231-4da9-82da-93c9a48ccb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "&search%5Bfilter_enum_constructie%5D%5B0%5D=inainte-de-1977&search%5Bfilter_enum_constructie%5D%5B1%5D=1977-1990&search%5Bfilter_enum_constructie%5D%5B2%5D=1990-2000&search%5Bfilter_enum_constructie%5D%5B3%5D=dupa-2000\n",
            "https://www.olx.ro/imobiliare/apartamente-garsoniere-de-vanzare/2-camere/iasi-judet/?currency=EUR&search%5Border%5D=created_at:desc&search%5Bfilter_float_m:from%5D=35&search%5Bfilter_float_m:to%5D=45&search%5Bfilter_enum_constructie%5D%5B0%5D=inainte-de-1977&search%5Bfilter_enum_constructie%5D%5B1%5D=1977-1990&search%5Bfilter_enum_constructie%5D%5B2%5D=1990-2000&search%5Bfilter_enum_constructie%5D%5B3%5D=dupa-2000&search%5Bfilter_enum_floor%5D%5B0%5D=parter&search%5Bfilter_enum_floor%5D%5B1%5D=fl_1&search%5Bfilter_enum_floor%5D%5B2%5D=fl_2&search%5Bfilter_enum_floor%5D%5B3%5D=fl_3&search%5Bfilter_enum_floor%5D%5B4%5D=fl_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    number_of_pages = get_number_of_pages(soup)\n",
        "    if not number_of_pages: \n",
        "        print(f\"Nici un rezult.\")\n",
        "    else: \n",
        "\n",
        "        print(f\"Number of pages: {number_of_pages}\")\n",
        "\n",
        "        rows = []\n",
        "        for pg_id in range(1, number_of_pages + 1):\n",
        "            print(f\"Extract info from page: {pg_id}\")\n",
        "            url = get_url(nr_camere_filter, etaj_url_filter, supr_min, supr_max, an_const_url_filter)\n",
        "            url += f\"&page={pg_id}\"\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                rows.extend(extract_info_from_page(soup))\n",
        "            else:\n",
        "                print(f\"Failed to retrieve page {pg_id}. Status code: {response.status_code}\")\n",
        "\n",
        "        print(\"Export info.\")\n",
        "        save_to_csv(rows, output_file_path)\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to retrieve the first page. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHguWUDve4nU",
        "outputId": "546f6e57-f5c1-4148-a799-7b85aa71f7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages: 11\n",
            "Extract info from page: 1\n",
            "Extract info from page: 2\n",
            "Extract info from page: 3\n",
            "Extract info from page: 4\n",
            "Extract info from page: 5\n",
            "Extract info from page: 6\n",
            "Extract info from page: 7\n",
            "Extract info from page: 8\n",
            "Extract info from page: 9\n",
            "Extract info from page: 10\n",
            "Extract info from page: 11\n",
            "Export info.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "from datetime import date\n",
        "\n",
        "def save_to_json(data, output_file_path):\n",
        "    def convert_to_serializable(obj):\n",
        "        if isinstance(obj, date):\n",
        "            return obj.strftime('%Y-%m-%d')\n",
        "        raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')\n",
        "\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(data, json_file, indent=4, default=convert_to_serializable)\n",
        "\n",
        "# Assuming you have the 'rows' list containing the extracted data\n",
        "output_file_pathJSON = os.path.join(script_directory, 'output.json')\n",
        "save_to_json(rows, output_file_pathJSON)"
      ],
      "metadata": {
        "id": "qQybpASrm-K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "dqNR__ynpdC_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}